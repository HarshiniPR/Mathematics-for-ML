import numpy as np
import matplotlib.pyplot as plt

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def log_likelihood(X, y, beta):
    z = X @ beta
    p = sigmoid(z)
    p = np.clip(p, 1e-12, 1 - 1e-12)
    return float(np.sum(y * np.log(p) + (1 - y) * np.log(1 - p)))

def gradient(X, y, beta):
    p = sigmoid(X @ beta)
    return X.T @ (y - p)

def hessian(X, beta):
    p = sigmoid(X @ beta).flatten()
    W = p * (1 - p)
    X_weighted = X * W[:, None]
    return X.T @ X_weighted

# Newton-Raphson Method
def newton_raphson_mle(X, y, initial_beta=None, max_iter=100, tol=1e-6):
    n_features = X.shape[1]
    if initial_beta is None:
        beta = np.zeros((n_features, 1))
    else:
        beta = initial_beta.copy()

    for i in range(max_iter):
        grad = gradient(X, y, beta)
        H = hessian(X, beta)

        try:
            delta = np.linalg.solve(H, grad)
        except np.linalg.LinAlgError:
            delta = np.linalg.pinv(H) @ grad

        beta_new = beta + delta
        if np.linalg.norm(beta_new - beta) < tol:
            beta = beta_new
            break
        beta = beta_new
    return beta

# Generate  data
np.random.seed(42)
n = 200
X_raw = np.random.rand(n, 1) * 10
true_beta = np.array([[-3.0], [2]])
X_b = np.c_[np.ones((n, 1)), X_raw]
probs = sigmoid(X_b @ true_beta)
y = (probs > np.random.rand(n, 1)).astype(float)

manual_beta = np.array([[-3.0], [2]])     

# Train -> MLE
initial_beta = np.zeros((2, 1))
estimated_beta = newton_raphson_mle(X_b, y, initial_beta)
print("Estimated beta (MLE):", estimated_beta.flatten())
print("Manual beta (user):", manual_beta.flatten())
acc_est = accuracy(X_b, y, estimated_beta)
acc_manual = accuracy(X_b, y, manual_beta)
print(f"Accuracy (estimated): {acc_est*100:.2f}%")
print(f"Accuracy (manual):    {acc_manual*100:.2f}%")
L_est = log_likelihood(X_b, y, estimated_beta)
L_manual = log_likelihood(X_b, y, manual_beta)

print("\nLog-Likelihood:")
print(f"MLE Beta Likelihood:    {L_est:.4f}")
print(f"Manual Beta Likelihood: {L_manual:.4f}")


#prediction
new_points = np.array([[1],[3],[7],[12]])        
new_points_b = np.c_[np.ones((new_points.shape[0],1)), new_points]  
new_probs = sigmoid(new_points_b @ estimated_beta)
new_classes = (new_probs >= 0.5).astype(int)

print("\nPrediction of data points:")
for x, p, c in zip(new_points, new_probs, new_classes):
    print(f"X ={x.item():>3} -> P(Y=1) = {p.item():.4f} -> Predicted Class = {c.item()}")

# Plot 
plt.figure(figsize=(10, 6))
plt.scatter(X_raw, y, c='blue', alpha=0.5, label="Data")

x_vals = np.linspace(-5, X_raw.max()+2, 300)
x_vals_b = np.c_[np.ones((300,1)), x_vals.reshape(-1,1)]
y_manual = sigmoid(x_vals_b @ manual_beta)
y_est = sigmoid(x_vals_b @ estimated_beta)

plt.plot(x_vals, y_manual, '--g', label=f"Manual β")
plt.plot(x_vals, y_est, '-r', label=f"MLE β")

plt.axvline(0, color='black', linewidth=0.6)
plt.axhline(0.5, color='gray', linestyle='--')

plt.xlabel("Feature X")
plt.ylabel("Probability (Y=1)")
plt.legend()
plt.grid(True)
plt.title("Manual vs MLE Logistic Regression Curve")
plt.show()
